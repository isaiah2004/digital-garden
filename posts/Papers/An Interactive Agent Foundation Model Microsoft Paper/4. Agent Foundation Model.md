They propose the below framework.

![[Pasted image 20240220014340.png]]

*Figure 3. Overview of our Interactive Agent framework. Our foundation model is designed to process multi-modal information that conveys various levels of abstraction. This approach facilitates a comprehensive understanding of the context and environment, thus ensuring that actions are coherent. By training on a variety of task domains and applications, we develop a versatile foundation model that can be fine-tuned for executing optimal actions in a variety of contexts, paving the way towards generally intelligent agents.*

By combining [[Visual perception]] and linguistic understanding. We can achieve a more intuitive understanding of their environment. --> Better contextual reasoning. 

Their work is on developing a joint image and video encoder and aligning this joint encoder with existing [[Foundation Modals]]

This has several notable benefits:
1. It allows for the use of both action, image and video with language.
2. Increases the capabilities of the model across a variety of downstream tasks. 
3. Decrease Overall model size.
4. Smaller size allows for edge deployments of modals.

### Model Architecture 
> To effectively initialize our model to handle text, visual, and agent tokens as input, we initialize our architecture with two pre-trained submodules. First, we use CLIP ViT-B16 from (Radford et al., 2021) to initialize our visual encoder, denoted Eθ, and initialize our action and language model, Fϕ, from OPT-125M (Zhang et al., 2022).

Two models to handle text and visual tokens:
1. [[CLIP ViT-B16]]
2. [[OPT-125M]] 

First they encode each frame in video V<sub>i</sub> as visual features. Z<sub>i</sub>  = E<sub>θ</sub>(V<sub>i</sub>). We enable cross-modal information sharing by training an additional layer $l$  that transforms the embeddings of our visual encoder $E_\theta$  ==>to Token embedding space of Transformer Model $F_\phi$. 

Thus we can obtain $\hat A$,  a text token or Action token.

